{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcdeb779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "84/84 [==============================] - 8s 18ms/step - loss: 0.0185\n",
      "Epoch 2/10\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 4.0266e-04\n",
      "Epoch 3/10\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 4.0308e-04\n",
      "Epoch 4/10\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 3.8157e-04\n",
      "Epoch 5/10\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 3.7853e-04\n",
      "Epoch 6/10\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 3.9504e-04\n",
      "Epoch 7/10\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 3.9471e-04\n",
      "Epoch 8/10\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 3.6105e-04\n",
      "Epoch 9/10\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 3.5409e-04\n",
      "Epoch 10/10\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 3.5389e-04\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 125\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m next_two_days\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 125\u001b[0m     \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 15\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSTOCK_INDEX.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     13\u001b[0m actual_close \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mloadtxt(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_close.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m pred_close \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Calculation of squared_error\u001b[39;00m\n\u001b[0;32m     18\u001b[0m actual_close \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(actual_close)\n",
      "Cell \u001b[1;32mIn[9], line 93\u001b[0m, in \u001b[0;36mpredict_func\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     90\u001b[0m     X_test_lstm\u001b[38;5;241m.\u001b[39mappend(inputs_lstm[i\u001b[38;5;241m-\u001b[39mlookback_lstm:i, \u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     92\u001b[0m X_test_lstm \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(X_test_lstm)\n\u001b[1;32m---> 93\u001b[0m X_test_lstm \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(X_test_lstm, (X_test_lstm\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[43mX_test_lstm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# Make predictions using LSTM model\u001b[39;00m\n\u001b[0;32m     97\u001b[0m predicted_prices_lstm \u001b[38;5;241m=\u001b[39m lstm_model\u001b[38;5;241m.\u001b[39mpredict(X_test_lstm)\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from arch import arch_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "ef = pd.read_csv('sample_input.csv')\n",
    "def evaluate():\n",
    "    df = pd.read_csv('STOCK_INDEX.csv')\n",
    "    \n",
    "    actual_close = np.loadtxt('sample_close.txt')\n",
    "    \n",
    "    pred_close = predict_func(df)\n",
    "    \n",
    "    # Calculation of squared_error\n",
    "    actual_close = np.array(actual_close)\n",
    "    pred_close = np.array(pred_close)\n",
    "    mean_square_error = np.mean(np.square(actual_close-pred_close))\n",
    "\n",
    "\n",
    "    pred_prev = [df['Close'].iloc[-1]]\n",
    "    pred_prev.append(pred_close[0])\n",
    "    pred_curr = pred_close\n",
    "    \n",
    "    actual_prev = [df['Close'].iloc[-1]]\n",
    "    actual_prev.append(actual_close[0])\n",
    "    actual_curr = actual_close\n",
    "\n",
    "    # Calculation of directional_accuracy\n",
    "    pred_dir = np.array(pred_curr)[:, 0] - np.array(pred_prev[1])\n",
    "    actual_dir = np.array(actual_curr) - np.array(actual_prev)\n",
    "    dir_accuracy = np.mean((pred_dir*actual_dir)>0)*100\n",
    "\n",
    "    print(f'Mean Square Error: {mean_square_error:.6f}\\nDirectional Accuracy: {dir_accuracy:.1f}')\n",
    "    \n",
    "def predict_func(df):\n",
    "    df.ffill(inplace=True)\n",
    "    # Prepare the DataFrame with log returns\n",
    "    df['log_returns'] = np.log(df['Close']).diff().dropna()\n",
    "    # Create a scaler to normalize the data\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_data = scaler.fit_transform(df['Close'].values.reshape(-1, 1))\n",
    "    # Split the data into training and test sets\n",
    "    train_size = int(len(scaled_data))\n",
    "    train_data = scaled_data[:train_size]\n",
    "#     test_data = scaled_data[train_size:]\n",
    "\n",
    "    # Prepare the training data for LSTM\n",
    "    X_train_lstm, y_train_lstm = [], []\n",
    "    lookback_lstm = 10  # Adjust the lookback window for LSTM\n",
    "\n",
    "    for i in range(lookback_lstm, len(train_data)):\n",
    "        X_train_lstm.append(train_data[i-lookback_lstm:i, 0])\n",
    "        y_train_lstm.append(train_data[i, 0])\n",
    "\n",
    "    X_train_lstm, y_train_lstm = np.array(X_train_lstm), np.array(y_train_lstm)\n",
    "\n",
    "    # Reshape the input data for LSTM\n",
    "    X_train_lstm = np.reshape(X_train_lstm, (X_train_lstm.shape[0], X_train_lstm.shape[1], 1))\n",
    "\n",
    "    # Create and fit the LSTM model\n",
    "    lstm_model = Sequential()\n",
    "    lstm_model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train_lstm.shape[1], 1)))\n",
    "    lstm_model.add(LSTM(units=50))\n",
    "    lstm_model.add(Dense(units=1))\n",
    "    lstm_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    lstm_model.fit(X_train_lstm, y_train_lstm, epochs=10, batch_size=32)\n",
    "\n",
    "    ef.ffill(inplace=True)\n",
    "    # Prepare the DataFrame with log returns\n",
    "    ef['log_returns'] = np.log(ef['Close']).diff().dropna()\n",
    "    # Create a scaler to normalize the data\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_data = scaler.fit_transform(ef['Close'].values.reshape(-1, 1))\n",
    "    # Split the data into training and test sets\n",
    "    test_size = int(len(scaled_data))\n",
    "#     train_data = scaled_data[:train_size]\n",
    "    test_data = scaled_data[:test_size]\n",
    "    \n",
    "    # Prepare the test data for LSTM\n",
    "    inputs_lstm = ef['Close'].values[len(ef) - len(test_data) - lookback_lstm:]\n",
    "    inputs_lstm = inputs_lstm.reshape(-1, 1)\n",
    "    inputs_lstm = scaler.transform(inputs_lstm)\n",
    "\n",
    "    X_test_lstm = []\n",
    "\n",
    "    for i in range(lookback_lstm, len(inputs_lstm)):\n",
    "        X_test_lstm.append(inputs_lstm[i-lookback_lstm:i, 0])\n",
    "    \n",
    "    X_test_lstm = np.array(X_test_lstm)\n",
    "    X_test_lstm = np.reshape(X_test_lstm, (X_test_lstm.shape[0], X_test_lstm.shape[1], 1))\n",
    "\n",
    "\n",
    "    # Make predictions using LSTM model\n",
    "    predicted_prices_lstm = lstm_model.predict(X_test_lstm)\n",
    "    predicted_prices_lstm = scaler.inverse_transform(predicted_prices_lstm)\n",
    "\n",
    "    # Prepare the test data for EMA\n",
    "    inputs_ema = ef['Close'].values[len(ef) - len(test_data) - lookback_lstm:]\n",
    "    inputs_ema = inputs_ema.reshape(-1, 1)\n",
    "    inputs_ema = scaler.transform(inputs_ema)\n",
    "\n",
    "    # Calculate EMA using alpha 0.75\n",
    "    ema = [inputs_ema[0]]  # Initialize the first EMA value as the first data point\n",
    "    alpha = 0.85\n",
    "    for i in range(1, len(inputs_ema)):\n",
    "        ema_value = alpha * inputs_ema[i] + (1 - alpha) * ema[i - 1]\n",
    "        ema.append(ema_value)\n",
    "\n",
    "    ema = np.array(ema)\n",
    "    ema = scaler.inverse_transform(ema)\n",
    "\n",
    "    # Adjust the shapes of predicted_prices_lstm and ema arrays\n",
    "    predicted_prices_lstm = predicted_prices_lstm[-len(ema):]\n",
    "\n",
    "    # Combine the predictions\n",
    "    combined_predictions = 0.05 * predicted_prices_lstm[-10:] + 0.95 * ema[-10:]\n",
    "    # Return the combined predictions for the next two days\n",
    "    next_two_days = combined_predictions[-2:]\n",
    "    return next_two_days\n",
    "\n",
    "if(__name__ == '__main__'):\n",
    "    evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4c147c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "84/84 [==============================] - 8s 22ms/step - loss: 0.0110\n",
      "Epoch 2/10\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 3.9604e-04\n",
      "Epoch 3/10\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 4.1955e-04\n",
      "Epoch 4/10\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 4.0243e-04\n",
      "Epoch 5/10\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 4.0805e-04\n",
      "Epoch 6/10\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 4.0548e-04\n",
      "Epoch 7/10\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 3.8226e-04\n",
      "Epoch 8/10\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 3.6397e-04\n",
      "Epoch 9/10\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 3.8894e-04\n",
      "Epoch 10/10\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 3.7934e-04\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unexpected result of `predict_function` (Empty batch_outputs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 109\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m next_two_days\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 109\u001b[0m     \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 12\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate\u001b[39m():\n\u001b[0;32m     11\u001b[0m     actual_close \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mloadtxt(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_close.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m     pred_close \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mef\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# Calculation of squared_error\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     mean_square_error \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39msquare(actual_close \u001b[38;5;241m-\u001b[39m pred_close))\n",
      "Cell \u001b[1;32mIn[11], line 82\u001b[0m, in \u001b[0;36mpredict_func\u001b[1;34m(train_df, test_df)\u001b[0m\n\u001b[0;32m     78\u001b[0m X_test_lstm \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(X_test_lstm, (X_test_lstm\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], lookback_lstm, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# Make predictions using LSTM model\u001b[39;00m\n\u001b[1;32m---> 82\u001b[0m predicted_prices_lstm \u001b[38;5;241m=\u001b[39m \u001b[43mlstm_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test_lstm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m predicted_prices_lstm \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39minverse_transform(predicted_prices_lstm)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Prepare the test data for EMA\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py:2579\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   2575\u001b[0m                 callbacks\u001b[38;5;241m.\u001b[39mon_predict_batch_end(\n\u001b[0;32m   2576\u001b[0m                     end_step, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: batch_outputs}\n\u001b[0;32m   2577\u001b[0m                 )\n\u001b[0;32m   2578\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batch_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2579\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2580\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected result of `predict_function` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2581\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(Empty batch_outputs). Please use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2582\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`Model.compile(..., run_eagerly=True)`, or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2583\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.config.run_functions_eagerly(True)` for more \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2584\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minformation of where went wrong, or file a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2585\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124missue/bug to `tf.keras`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2586\u001b[0m         )\n\u001b[0;32m   2587\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_predict_end()\n\u001b[0;32m   2588\u001b[0m all_outputs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mmap_structure_up_to(\n\u001b[0;32m   2589\u001b[0m     batch_outputs, potentially_ragged_concat, outputs\n\u001b[0;32m   2590\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: Unexpected result of `predict_function` (Empty batch_outputs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "df = pd.read_csv('STOCK_INDEX.csv')\n",
    "ef = pd.read_csv('sample_input.csv')\n",
    "\n",
    "def evaluate():\n",
    "    actual_close = np.loadtxt('sample_close.txt')\n",
    "    pred_close = predict_func(df, ef)\n",
    "\n",
    "    # Calculation of squared_error\n",
    "    mean_square_error = np.mean(np.square(actual_close - pred_close))\n",
    "\n",
    "    pred_prev = [df['Close'].iloc[-1]]\n",
    "    pred_prev.append(pred_close[0])\n",
    "    pred_curr = pred_close\n",
    "\n",
    "    actual_prev = [df['Close'].iloc[-1]]\n",
    "    actual_prev.append(actual_close[0])\n",
    "    actual_curr = actual_close\n",
    "\n",
    "    # Calculation of directional_accuracy\n",
    "    pred_dir = np.array(pred_curr)[:, 0] - np.array(pred_prev[1])\n",
    "    actual_dir = np.array(actual_curr) - np.array(actual_prev)\n",
    "    dir_accuracy = np.mean((pred_dir * actual_dir) > 0) * 100\n",
    "\n",
    "    print(f'Mean Square Error: {mean_square_error:.6f}\\nDirectional Accuracy: {dir_accuracy:.1f}')\n",
    "\n",
    "def predict_func(train_df, test_df):\n",
    "    train_df.ffill(inplace=True)\n",
    "    # Prepare the DataFrame with log returns\n",
    "    train_df['log_returns'] = np.log(train_df['Close']).diff().dropna()\n",
    "    # Create a scaler to normalize the data\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_train_data = scaler.fit_transform(train_df['Close'].values.reshape(-1, 1))\n",
    "\n",
    "    # Prepare the training data for LSTM\n",
    "    X_train_lstm, y_train_lstm = [], []\n",
    "    lookback_lstm = 10  # Adjust the lookback window for LSTM\n",
    "\n",
    "    for i in range(lookback_lstm, len(scaled_train_data)):\n",
    "        X_train_lstm.append(scaled_train_data[i - lookback_lstm:i, 0])\n",
    "        y_train_lstm.append(scaled_train_data[i, 0])\n",
    "\n",
    "    X_train_lstm, y_train_lstm = np.array(X_train_lstm), np.array(y_train_lstm)\n",
    "\n",
    "    # Reshape the input data for LSTM\n",
    "    X_train_lstm = np.reshape(X_train_lstm, (X_train_lstm.shape[0], X_train_lstm.shape[1], 1))\n",
    "\n",
    "    # Create and fit the LSTM model\n",
    "    lstm_model = Sequential()\n",
    "    lstm_model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train_lstm.shape[1], 1)))\n",
    "    lstm_model.add(LSTM(units=50))\n",
    "    lstm_model.add(Dense(units=1))\n",
    "    lstm_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    lstm_model.fit(X_train_lstm, y_train_lstm, epochs=10, batch_size=32)\n",
    "\n",
    "    test_df.ffill(inplace=True)\n",
    "    # Prepare the DataFrame with log returns\n",
    "    test_df['log_returns'] = np.log(test_df['Close']).diff().dropna()\n",
    "    # Normalize the test data using the scaler from training\n",
    "    scaled_test_data = scaler.transform(test_df['Close'].values.reshape(-1, 1))\n",
    "\n",
    "    # Prepare the test data for LSTM\n",
    "    inputs_lstm = scaled_test_data[len(test_df) - lookback_lstm:]\n",
    "    inputs_lstm = inputs_lstm.reshape(-1, 1)\n",
    "    inputs_lstm = scaler.transform(inputs_lstm)\n",
    "\n",
    "    X_test_lstm = []\n",
    "\n",
    "    for i in range(lookback_lstm, len(inputs_lstm)):\n",
    "        X_test_lstm.append(inputs_lstm[i - lookback_lstm:i, 0])\n",
    "\n",
    "    X_test_lstm = np.array(X_test_lstm)\n",
    "    X_test_lstm = np.reshape(X_test_lstm, (X_test_lstm.shape[0], lookback_lstm, 1))\n",
    "\n",
    "\n",
    "    # Make predictions using LSTM model\n",
    "    predicted_prices_lstm = lstm_model.predict(X_test_lstm)\n",
    "    predicted_prices_lstm = scaler.inverse_transform(predicted_prices_lstm)\n",
    "\n",
    "    # Prepare the test data for EMA\n",
    "    inputs_ema = scaled_test_data[len(test_df) - lookback_lstm:]\n",
    "    inputs_ema = inputs_ema.reshape(-1, 1)\n",
    "\n",
    "    # Calculate EMA using alpha 0.75\n",
    "    ema = [inputs_ema[0]]  # Initialize the first EMA value as the first data point\n",
    "    alpha = 0.85\n",
    "    for i in range(1, len(inputs_ema)):\n",
    "        ema_value = alpha * inputs_ema[i] + (1 - alpha) * ema[i - 1]\n",
    "        ema.append(ema_value)\n",
    "\n",
    "    ema = np.array(ema)\n",
    "    ema = scaler.inverse_transform(ema)\n",
    "\n",
    "    # Adjust the shapes of predicted_prices_lstm and ema arrays\n",
    "    predicted_prices_lstm = predicted_prices_lstm[-len(ema):]\n",
    "\n",
    "    # Combine the predictions\n",
    "    combined_predictions = 0.05 * predicted_prices_lstm[-10:, 0] + 0.95 * ema[-10:, 0]\n",
    "    # Return the combined predictions for the next two days\n",
    "    next_two_days = combined_predictions[-2:]\n",
    "    return next_two_days\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aef7ff72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "84/84 [==============================] - 8s 20ms/step - loss: 0.0087\n",
      "Epoch 2/10\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 3.8722e-04\n",
      "Epoch 3/10\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 3.8972e-04\n",
      "Epoch 4/10\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 3.6669e-04\n",
      "Epoch 5/10\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 3.6663e-04\n",
      "Epoch 6/10\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 3.6493e-04\n",
      "Epoch 7/10\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 3.6543e-04\n",
      "Epoch 8/10\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 3.5002e-04\n",
      "Epoch 9/10\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 3.5857e-04\n",
      "Epoch 10/10\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 3.5537e-04\n",
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 99\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m next_two_days\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 99\u001b[0m     \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 26\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m actual_curr \u001b[38;5;241m=\u001b[39m actual_close\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Calculation of directional_accuracy\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m pred_dir \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_curr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39marray(pred_prev[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     27\u001b[0m actual_dir \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(actual_curr) \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39marray(actual_prev)\n\u001b[0;32m     28\u001b[0m dir_accuracy \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean((pred_dir \u001b[38;5;241m*\u001b[39m actual_dir) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "41e0d471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "42/42 [==============================] - 9s 43ms/step - loss: 0.0185\n",
      "Epoch 2/100\n",
      "42/42 [==============================] - 2s 43ms/step - loss: 5.3149e-04\n",
      "Epoch 3/100\n",
      "42/42 [==============================] - 2s 42ms/step - loss: 3.7185e-04\n",
      "Epoch 4/100\n",
      "42/42 [==============================] - 2s 42ms/step - loss: 3.6647e-04\n",
      "Epoch 5/100\n",
      "42/42 [==============================] - 2s 41ms/step - loss: 3.6393e-04\n",
      "Epoch 6/100\n",
      "42/42 [==============================] - 2s 41ms/step - loss: 3.6480e-04\n",
      "Epoch 7/100\n",
      "42/42 [==============================] - 2s 41ms/step - loss: 3.6123e-04\n",
      "Epoch 8/100\n",
      "42/42 [==============================] - 2s 41ms/step - loss: 3.6566e-04\n",
      "Epoch 9/100\n",
      "42/42 [==============================] - 2s 42ms/step - loss: 3.6192e-04\n",
      "Epoch 10/100\n",
      "42/42 [==============================] - 2s 41ms/step - loss: 3.7323e-04\n",
      "Epoch 11/100\n",
      "42/42 [==============================] - 2s 42ms/step - loss: 3.4945e-04\n",
      "Epoch 12/100\n",
      "42/42 [==============================] - 2s 41ms/step - loss: 3.5274e-04\n",
      "Epoch 13/100\n",
      "42/42 [==============================] - 2s 41ms/step - loss: 3.4433e-04\n",
      "Epoch 14/100\n",
      "42/42 [==============================] - 2s 41ms/step - loss: 3.4404e-04\n",
      "Epoch 15/100\n",
      "42/42 [==============================] - 2s 41ms/step - loss: 3.5146e-04\n",
      "Epoch 16/100\n",
      "42/42 [==============================] - 2s 41ms/step - loss: 3.3903e-04\n",
      "Epoch 17/100\n",
      "42/42 [==============================] - 2s 41ms/step - loss: 3.4368e-04\n",
      "Epoch 18/100\n",
      "42/42 [==============================] - 2s 41ms/step - loss: 3.3170e-04\n",
      "Epoch 19/100\n",
      "42/42 [==============================] - 2s 41ms/step - loss: 3.4777e-04\n",
      "Epoch 20/100\n",
      "42/42 [==============================] - 2s 41ms/step - loss: 3.1847e-04\n",
      "Epoch 21/100\n",
      "42/42 [==============================] - 2s 42ms/step - loss: 3.3631e-04\n",
      "Epoch 22/100\n",
      "42/42 [==============================] - 2s 43ms/step - loss: 3.2286e-04\n",
      "Epoch 23/100\n",
      "42/42 [==============================] - 2s 42ms/step - loss: 3.1909e-04\n",
      "Epoch 24/100\n",
      "42/42 [==============================] - 2s 42ms/step - loss: 3.1856e-04\n",
      "Epoch 25/100\n",
      "42/42 [==============================] - 2s 42ms/step - loss: 3.0421e-04\n",
      "Epoch 26/100\n",
      "42/42 [==============================] - 2s 42ms/step - loss: 2.8279e-04\n",
      "Epoch 27/100\n",
      "42/42 [==============================] - 2s 41ms/step - loss: 2.9551e-04\n",
      "Epoch 28/100\n",
      "42/42 [==============================] - 2s 41ms/step - loss: 2.9583e-04\n",
      "Epoch 29/100\n",
      "42/42 [==============================] - 2s 41ms/step - loss: 2.6974e-04\n",
      "Epoch 30/100\n",
      "42/42 [==============================] - 2s 41ms/step - loss: 3.2503e-04\n",
      "Epoch 31/100\n",
      "42/42 [==============================] - 2s 41ms/step - loss: 2.8466e-04\n",
      "Epoch 32/100\n",
      "42/42 [==============================] - 2s 42ms/step - loss: 2.6124e-04\n",
      "Epoch 33/100\n",
      "42/42 [==============================] - 2s 41ms/step - loss: 2.4819e-04\n",
      "Epoch 34/100\n",
      "42/42 [==============================] - 2s 42ms/step - loss: 2.6771e-04\n",
      "Epoch 35/100\n",
      "42/42 [==============================] - 2s 41ms/step - loss: 2.5445e-04\n",
      "Epoch 36/100\n",
      "42/42 [==============================] - 2s 41ms/step - loss: 2.4694e-04\n",
      "Epoch 37/100\n",
      "42/42 [==============================] - 2s 41ms/step - loss: 2.5829e-04\n",
      "Epoch 38/100\n",
      "42/42 [==============================] - 2s 41ms/step - loss: 2.7771e-04\n",
      "Epoch 39/100\n",
      "42/42 [==============================] - 2s 41ms/step - loss: 2.4559e-04\n",
      "Epoch 40/100\n",
      "42/42 [==============================] - 2s 41ms/step - loss: 2.3768e-04\n",
      "Epoch 41/100\n",
      "42/42 [==============================] - 2s 41ms/step - loss: 2.2575e-04\n",
      "Epoch 42/100\n",
      "42/42 [==============================] - 2s 41ms/step - loss: 2.2913e-04\n",
      "Epoch 43/100\n",
      "42/42 [==============================] - 2s 41ms/step - loss: 2.1903e-04\n",
      "Epoch 44/100\n",
      "42/42 [==============================] - 2s 41ms/step - loss: 2.1505e-04\n",
      "Epoch 45/100\n",
      "42/42 [==============================] - 2s 41ms/step - loss: 2.0221e-04\n",
      "Epoch 46/100\n",
      "42/42 [==============================] - 2s 41ms/step - loss: 1.9476e-04\n",
      "Epoch 47/100\n",
      "42/42 [==============================] - 2s 41ms/step - loss: 2.0605e-04\n",
      "Epoch 48/100\n",
      "42/42 [==============================] - 2s 41ms/step - loss: 2.0101e-04\n",
      "Epoch 49/100\n",
      "42/42 [==============================] - 2s 41ms/step - loss: 2.1651e-04\n",
      "Epoch 50/100\n",
      "42/42 [==============================] - 2s 41ms/step - loss: 2.0914e-04\n",
      "Epoch 51/100\n",
      "42/42 [==============================] - 2s 41ms/step - loss: 1.8616e-04\n",
      "Epoch 52/100\n",
      "42/42 [==============================] - 2s 41ms/step - loss: 2.0401e-04\n",
      "Epoch 53/100\n",
      "42/42 [==============================] - 2s 41ms/step - loss: 2.1820e-04\n",
      "Epoch 54/100\n",
      "42/42 [==============================] - 2s 41ms/step - loss: 1.8016e-04\n",
      "Epoch 55/100\n",
      "42/42 [==============================] - 2s 41ms/step - loss: 1.8541e-04\n",
      "Epoch 56/100\n",
      "42/42 [==============================] - 2s 41ms/step - loss: 1.7376e-04\n",
      "Epoch 57/100\n",
      "42/42 [==============================] - 2s 41ms/step - loss: 1.7859e-04\n",
      "Epoch 58/100\n",
      "42/42 [==============================] - 2s 41ms/step - loss: 1.8869e-04\n",
      "Epoch 59/100\n",
      "42/42 [==============================] - 2s 41ms/step - loss: 1.6858e-04\n",
      "Epoch 60/100\n",
      "42/42 [==============================] - 2s 41ms/step - loss: 1.7447e-04\n",
      "Epoch 61/100\n",
      "42/42 [==============================] - 2s 41ms/step - loss: 1.7360e-04\n",
      "Epoch 62/100\n",
      "42/42 [==============================] - 2s 41ms/step - loss: 1.6470e-04\n",
      "Epoch 63/100\n",
      "42/42 [==============================] - 2s 41ms/step - loss: 1.5545e-04\n",
      "Epoch 64/100\n",
      "42/42 [==============================] - 2s 41ms/step - loss: 1.6388e-04\n",
      "Epoch 65/100\n",
      "42/42 [==============================] - 2s 42ms/step - loss: 1.7305e-04\n",
      "Epoch 66/100\n",
      "42/42 [==============================] - 1s 33ms/step - loss: 1.5471e-04\n",
      "Epoch 67/100\n",
      "42/42 [==============================] - 2s 46ms/step - loss: 1.6545e-04\n",
      "Epoch 68/100\n",
      "42/42 [==============================] - 2s 46ms/step - loss: 1.4770e-04\n",
      "Epoch 69/100\n",
      "42/42 [==============================] - 2s 46ms/step - loss: 1.8068e-04\n",
      "Epoch 70/100\n",
      "42/42 [==============================] - 2s 45ms/step - loss: 1.5398e-04\n",
      "Epoch 71/100\n",
      "42/42 [==============================] - 2s 46ms/step - loss: 1.8989e-04\n",
      "Epoch 72/100\n",
      "42/42 [==============================] - 2s 45ms/step - loss: 1.5552e-04\n",
      "Epoch 73/100\n",
      "42/42 [==============================] - 2s 47ms/step - loss: 1.6266e-04\n",
      "Epoch 74/100\n",
      "42/42 [==============================] - 2s 44ms/step - loss: 1.4805e-04\n",
      "Epoch 75/100\n",
      "42/42 [==============================] - 2s 43ms/step - loss: 1.5571e-04\n",
      "Epoch 76/100\n",
      "42/42 [==============================] - 2s 43ms/step - loss: 1.3519e-04\n",
      "Epoch 77/100\n",
      "42/42 [==============================] - 2s 45ms/step - loss: 1.3702e-04\n",
      "Epoch 78/100\n",
      "42/42 [==============================] - 2s 41ms/step - loss: 1.4072e-04\n",
      "Epoch 79/100\n",
      "42/42 [==============================] - 2s 40ms/step - loss: 1.3786e-04\n",
      "Epoch 80/100\n",
      "42/42 [==============================] - 2s 41ms/step - loss: 1.7642e-04\n",
      "Epoch 81/100\n",
      "42/42 [==============================] - 2s 43ms/step - loss: 1.5967e-04\n",
      "Epoch 82/100\n",
      "42/42 [==============================] - 2s 43ms/step - loss: 1.4736e-04\n",
      "Epoch 83/100\n",
      "42/42 [==============================] - 2s 42ms/step - loss: 1.3268e-04\n",
      "Epoch 84/100\n",
      "42/42 [==============================] - 2s 42ms/step - loss: 1.3011e-04\n",
      "Epoch 85/100\n",
      "42/42 [==============================] - 2s 44ms/step - loss: 1.3038e-04\n",
      "Epoch 86/100\n",
      "42/42 [==============================] - 2s 42ms/step - loss: 1.2576e-04\n",
      "Epoch 87/100\n",
      "42/42 [==============================] - 2s 43ms/step - loss: 1.3338e-04\n",
      "Epoch 88/100\n",
      "42/42 [==============================] - 2s 42ms/step - loss: 1.1848e-04\n",
      "Epoch 89/100\n",
      "42/42 [==============================] - 2s 42ms/step - loss: 1.3477e-04\n",
      "Epoch 90/100\n",
      "42/42 [==============================] - 2s 42ms/step - loss: 1.5447e-04\n",
      "Epoch 91/100\n",
      "42/42 [==============================] - 2s 41ms/step - loss: 1.2482e-04\n",
      "Epoch 92/100\n",
      "42/42 [==============================] - 2s 42ms/step - loss: 1.3054e-04\n",
      "Epoch 93/100\n",
      "42/42 [==============================] - 2s 41ms/step - loss: 1.2543e-04\n",
      "Epoch 94/100\n",
      "42/42 [==============================] - 2s 43ms/step - loss: 1.4317e-04\n",
      "Epoch 95/100\n",
      "42/42 [==============================] - 2s 42ms/step - loss: 1.2176e-04\n",
      "Epoch 96/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 2s 42ms/step - loss: 1.1548e-04\n",
      "Epoch 97/100\n",
      "42/42 [==============================] - 2s 45ms/step - loss: 1.1966e-04\n",
      "Epoch 98/100\n",
      "42/42 [==============================] - 2s 45ms/step - loss: 1.2439e-04\n",
      "Epoch 99/100\n",
      "42/42 [==============================] - 2s 47ms/step - loss: 1.2920e-04\n",
      "Epoch 100/100\n",
      "42/42 [==============================] - 2s 50ms/step - loss: 1.1501e-04\n",
      "INFO:tensorflow:Assets written to: model_final.m1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_final.m1\\assets\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "df = pd.read_csv('STOCK_INDEX.csv')\n",
    "ef = pd.read_csv('sample_input.csv')\n",
    "\n",
    "def evaluate():\n",
    "    actual_close = np.loadtxt('sample_close.txt')\n",
    "    pred_close = predict_func(df, ef)\n",
    "\n",
    "    # Calculation of squared_error\n",
    "#     mean_square_error = np.mean(np.square(actual_close - pred_close))\n",
    "\n",
    "#     pred_prev = [df['Close'].iloc[-1]]\n",
    "#     pred_prev.append(pred_close[0])\n",
    "#     pred_curr = pred_close\n",
    "\n",
    "#     actual_prev = [df['Close'].iloc[-1]]\n",
    "#     actual_prev.append(actual_close[0])\n",
    "#     actual_curr = actual_close\n",
    "\n",
    "#     # Calculation of directional_accuracy\n",
    "#     pred_dir = np.array(pred_curr) - np.array(pred_prev[1])\n",
    "#     actual_dir = np.array(actual_curr) - np.array(actual_prev)\n",
    "#     dir_accuracy = np.mean((pred_dir * actual_dir) > 0) * 100\n",
    "\n",
    "#     print(f'Mean Square Error: {mean_square_error:.6f}\\nDirectional Accuracy: {dir_accuracy:.1f}')\n",
    "\n",
    "def predict_func(train_df, test_df):\n",
    "    \n",
    "    train_df.ffill(inplace=True)\n",
    "    # Prepare the DataFrame with log returns\n",
    "    train_df['log_returns'] = np.log(train_df['Close']).diff().dropna()\n",
    "    # Create a scaler to normalize the data\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_train_data = scaler.fit_transform(train_df['Close'].values.reshape(-1, 1))\n",
    "\n",
    "    # Prepare the training data for LSTM\n",
    "    X_train_lstm, y_train_lstm = [], []\n",
    "    lookback_lstm = 10  # Adjust the lookback window for LSTM\n",
    "\n",
    "    for i in range(lookback_lstm, len(scaled_train_data)):\n",
    "        X_train_lstm.append(scaled_train_data[i - lookback_lstm:i, 0])\n",
    "        y_train_lstm.append(scaled_train_data[i, 0])\n",
    "\n",
    "    X_train_lstm, y_train_lstm = np.array(X_train_lstm), np.array(y_train_lstm)\n",
    "\n",
    "    # Reshape the input data for LSTM\n",
    "    X_train_lstm = np.reshape(X_train_lstm, (X_train_lstm.shape[0], X_train_lstm.shape[1], 1))\n",
    "\n",
    "    # Create and fit the LSTM model\n",
    "    lstm_model = Sequential()\n",
    "    lstm_model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train_lstm.shape[1], 1)))\n",
    "    lstm_model.add(LSTM(units=50))\n",
    "    lstm_model.add(Dense(units=1))\n",
    "    lstm_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    lstm_model.fit(X_train_lstm, y_train_lstm, epochs=100, batch_size=64)\n",
    "    \n",
    "    lstm_model.save('model_final.m1')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d606cf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.ffill(inplace=True)\n",
    "    # Prepare the DataFrame with log returns\n",
    "    test_df['log_returns'] = np.log(test_df['Close']).diff().dropna()\n",
    "    # Normalize the test data using the scaler from training\n",
    "    scaled_test_data = scaler.transform(test_df['Close'].values.reshape(-1, 1))\n",
    "\n",
    "    # Prepare the test data for LSTM\n",
    "    inputs_lstm = scaled_test_data[-lookback_lstm:]\n",
    "    inputs_lstm = inputs_lstm.reshape(1, -1, 1)\n",
    "\n",
    "    # Make predictions using LSTM model\n",
    "    predicted_prices_lstm = lstm_model.predict(inputs_lstm)\n",
    "    predicted_prices_lstm = scaler.inverse_transform(predicted_prices_lstm)\n",
    "\n",
    "    # Prepare the test data for EMA\n",
    "    inputs_ema = scaled_test_data[-lookback_lstm:]\n",
    "    inputs_ema = inputs_ema.reshape(-1, 1)\n",
    "\n",
    "    # Calculate EMA using alpha 0.75\n",
    "    ema = [inputs_ema[0]]  # Initialize the first EMA value as the first data point\n",
    "    alpha = 0.85\n",
    "    for i in range(1, len(inputs_ema)):\n",
    "        ema_value = alpha * inputs_ema[i] + (1 - alpha) * ema[i - 1]\n",
    "        ema.append(ema_value)\n",
    "\n",
    "    ema = np.array(ema)\n",
    "    ema = scaler.inverse_transform(ema)\n",
    "\n",
    "    # Adjust the shapes of predicted_prices_lstm and ema arrays\n",
    "    predicted_prices_lstm = predicted_prices_lstm[-len(ema):]\n",
    "\n",
    "    # Combine the predictions\n",
    "    combined_predictions = 0.05 * predicted_prices_lstm[-10:, 0] + 0.95 * ema[-10:, 0]\n",
    "    # Return the combined predictions for the next two days\n",
    "    next_two_days = combined_predictions[-2:]\n",
    "    return next_two_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f0de7558",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m pred_prev \u001b[38;5;241m=\u001b[39m [df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClose\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]]\n\u001b[0;32m      2\u001b[0m pred_prev\n\u001b[1;32m----> 3\u001b[0m \u001b[43mpred_prev\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "    pred_prev = [df['Close'].iloc[-1]]\n",
    "    pred_prev\n",
    "    pred_prev.size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43e5b32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
